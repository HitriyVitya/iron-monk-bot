import asyncio
import requests
import re
import database as db
from bs4 import BeautifulSoup

CHANNELS = [
    "shadowsockskeys", "oneclickvpnkeys", "v2ray_outlineir", "v2ray_free_conf", 
    "VlessConfig", "PrivateVPNs", "gurvpn_keys", "vmessh", "VMESS7"
]

# –°–∫–æ–ª—å–∫–æ –°–¢–†–ê–ù–ò–¶ –∏—Å—Ç–æ—Ä–∏–∏ –≤—ã–∫–∞—á–∞—Ç—å –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ (–ø–æ—Ç–æ–º —Å–Ω–∏–∑–∏–º)
PAGES_DEPTH = 30 

async def scrape_all():
    """–í—ã—Å–∞—Å—ã–≤–∞–µ—Ç –≤–æ–æ–±—â–µ –≤—Å—ë –∏–∑ –¢–ì"""
    pattern = re.compile(r'(?:vless|vmess|ss|trojan|hy2|tuic)://[^\s<"\'\)]+')
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    for channel in CHANNELS:
        url = f"https://t.me/s/{channel}"
        print(f"üì° –ì–ª—É–±–æ–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥: {channel}")
        
        for _ in range(PAGES_DEPTH):
            try:
                resp = requests.get(url, headers=headers, timeout=10)
                soup = BeautifulSoup(resp.text, 'html.parser')
                matches = pattern.findall(resp.text)
                
                if matches:
                    db.add_to_warehouse(matches)
                
                # –õ–∏—Å—Ç–∞–µ–º –Ω–∞–∑–∞–¥
                more = soup.find('a', class_='tme_messages_more')
                if more and 'href' in more.attrs:
                    url = "https://t.me" + more['href']
                    await asyncio.sleep(1) # –ù–µ —á–∞—Å—Ç–∏–º, —á—Ç–æ–±—ã –¢–ì –Ω–µ –∑–∞–±–∞–Ω–∏–ª
                else: break
            except: break

async def heavy_checker():
    """–ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—á–∫–∞–º–∏ —á–µ—Ä–µ–∑ Sing-box"""
    while True:
        to_check = db.get_next_proxies_to_check(10) # –ë–µ—Ä–µ–º –ø–æ 10 —à—Ç—É–∫
        if not to_check:
            await asyncio.sleep(60)
            continue
            
        for url in to_check:
            # –¢–£–¢ –ë–£–î–ï–¢ –í–´–ó–û–í SING-BOX (–¥–æ–±–∞–≤–∏–º —Å–ª–µ–¥—É—é—â–∏–º —à–∞–≥–æ–º)
            # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∞ —Å–∏—Å—Ç–µ–º—ã
            db.update_proxy_status(url, True, 100, 0) 
            await asyncio.sleep(1)

async def proxy_worker():
    # 1. –ü–µ—Ä–≤—ã–π —Ä–∞–∑ –≤—ã—Å–∞—Å—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
    await scrape_all()
    # 2. –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ—á–Ω—ã–π —á–µ–∫–µ—Ä
    await heavy_checker()
