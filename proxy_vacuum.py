import asyncio
import requests
import re
import base64
import json
import time
import logging
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import database_vpn as db
import keep_alive
# --- –°–ü–ò–°–ö–ò ---
TG_CHANNELS = [
    "shadowsockskeys", "oneclickvpnkeys", "v2ray_outlineir",
    "v2ray_free_conf", "v2rayngvpn", "v2ray_free_vpn",
    "gurvpn_keys", "vmessh", "VMESS7", "VlessConfig",
    "PrivateVPNs", "nV_v2ray", "NotorVPN", "FairVpn_V2ray",
    "outline_marzban", "outline_k"
]

EXTERNAL_SUBS = [
    "https://raw.githubusercontent.com/yebekhe/TelegramV2rayCollector/main/sub/normal/mix",
    "https://raw.githubusercontent.com/vfarid/v2ray-share/main/all_v2ray_configs.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub1.txt",
    "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/sub/sub_merge.txt",
    "https://raw.githubusercontent.com/LonUp/NodeList/main/NodeList.txt",
    "https://raw.githubusercontent.com/officialputuid/V2Ray-Config/main/Splitted-v2ray-config/all"
]

def safe_decode(s):
    try:
        s = re.sub(r'[^a-zA-Z0-9+/=]', '', s)
        padding = len(s) % 4
        if padding: s += '=' * (4 - padding)
        return base64.b64decode(s).decode('utf-8', errors='ignore')
    except: return ""

def extract_ip_port(link):
    try:
        if link.startswith("vmess://"):
            data = json.loads(safe_decode(link[8:]))
            return data.get('add'), int(data.get('port'))
        p = urlparse(link)
        if link.startswith("ss://") and "@" in link:
            part = link.split("@")[-1].split("#")[0].split("/")[0]
            if ":" in part: 
                return part.split(":")[0].replace("[","").replace("]",""), int(part.split(":")[1])
        if p.hostname and p.port: return p.hostname, p.port
    except: pass
    return None, None

async def check_tcp(ip, port):
    try:
        st = time.time()
        conn = asyncio.open_connection(ip, port)
        _, w = await asyncio.wait_for(conn, timeout=1.2) # –ñ–µ—Å—Ç–∫–∏–π —Ç–∞–π–º–∞—É—Ç 1.2 —Å–µ–∫
        lat = int((time.time() - st) * 1000)
        w.close()
        await w.wait_closed()
        return lat
    except: return None

# --- –ó–ê–î–ê–ß–ê 1: –ü–´–õ–ï–°–û–° (–°–æ—Å–µ—Ç –∏ —Å—Ä–∞–∑—É —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç) ---
async def scraper_task():
    regex = re.compile(r'(?:vless|vmess|ss|ssr|trojan|hy2|hysteria|hysteria2|tuic|socks5)://[^\s<"\'\)]+')
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    while True:
        logging.info("üì• [Scraper] –°—Ç–∞—Ä—Ç —Ü–∏–∫–ª–∞ —Å–±–æ—Ä–∞...")
        
        # 1. –ì–ò–¢–•–ê–ë (–ë—ã—Å—Ç—Ä–æ)
        for url in EXTERNAL_SUBS:
            try:
                r = requests.get(url, headers=headers, timeout=15)
                text = r.text
                if len(text) > 20 and not "://" in text[:50]:
                    d = safe_decode(text)
                    if d: text = d
                
                batch = []
                for l in regex.findall(text): batch.append(l.strip())
                
                if batch:
                    count = db.save_proxy_batch(batch)
                    if count > 0: logging.info(f"üì• [Scraper] +{count} —Å –ì–∏—Ç—Ö–∞–±–∞")
            except: pass
            await asyncio.sleep(1) # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        # 2. –¢–ï–õ–ï–ì–†–ê–ú (–ú–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –º–Ω–æ–≥–æ)
        for ch in TG_CHANNELS:
            url = f"https://t.me/s/{ch}"
            for _ in range(5): # –õ–∏—Å—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ 5 —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ —Ä–∞–∑, —á—Ç–æ–±—ã –Ω–µ –≤–∏—Å–Ω—É—Ç—å
                try:
                    r = requests.get(url, headers=headers, timeout=5)
                    soup_text = r.text
                    
                    batch = []
                    for l in regex.findall(soup_text):
                        batch.append(l.strip().split('<')[0])
                    
                    if batch:
                        count = db.save_proxy_batch(batch)
                        # logging.info(f"üì• [Scraper] +{count} —Å {ch}") # –ú–æ–∂–Ω–æ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ç—å –¥–ª—è –¥–µ–±–∞–≥–∞
                    
                    if 'tme_messages_more' in soup_text:
                        match = re.search(r'href="(/s/.*?)"', soup_text)
                        if match: url = "https://t.me" + match.group(1)
                        else: break
                    else: break
                    await asyncio.sleep(0.5)
                except: break
        
        logging.info("üí§ [Scraper] –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à–µ–Ω. –°–ø–ª—é 30 –º–∏–Ω—É—Ç.")
        await asyncio.sleep(1800)

# --- –ó–ê–î–ê–ß–ê 2: –ß–ï–ö–ï–† (–ë–µ—Ä–µ—Ç –∏–∑ –±–∞–∑—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç) ---
async def checker_task():
    while True:
        # –ë–µ—Ä–µ–º 100 –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏–ª–∏ —Å—Ç–∞—Ä—ã—Ö
        candidates = db.get_proxies_to_check(limit=100)
        
        if not candidates:
            # –ï—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –Ω–µ—á–µ–≥–æ, —Å–ø–∏–º —á—É—Ç—å-—á—É—Ç—å –∏ –∂–¥–µ–º –ø—ã–ª–µ—Å–æ—Å
            await asyncio.sleep(10)
            continue
            
        # logging.info(f"üß™ [Checker] –ü—Ä–æ–≤–µ—Ä—è—é –ø–∞—á–∫—É –∏–∑ {len(candidates)}...")
        
        sem = asyncio.Semaphore(50) # 50 –ø–æ—Ç–æ–∫–æ–≤
        
        async def verify(url):
            async with sem:
                ip, port = extract_ip_port(url)
                if not ip or not port:
                    db.update_proxy_status(url, None, 0, "")
                    return

                lat = await check_tcp(ip, port)
                if lat:
                    # AI (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
                    is_ai = 1 if "reality" in url.lower() or "pbk=" in url.lower() else 0
                    db.update_proxy_status(url, lat, is_ai, "üè≥Ô∏è")
                else:
                    db.update_proxy_status(url, None, 0, "")

        await asyncio.gather(*(verify(u) for u in candidates))
        # ... –≤ –∫–æ–Ω—Ü–µ —Ñ—É–Ω–∫—Ü–∏–∏ checker_task, –ø–æ—Å–ª–µ await asyncio.gather ...
        await asyncio.gather(*(verify(u) for u in candidates))
        
   
            # –í–´–ó–´–í–ê–ï–ú –û–ë–ù–û–í–õ–ï–ù–ò–ï –ö–≠–®–ê
        import keep_alive
        keep_alive.update_internal_cache()
        await asyncio.sleep(2)

# --- –ó–ê–ü–£–°–ö ---
async def vacuum_job():
    # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–≤–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Ü–∏–∫–ª–∞
    asyncio.create_task(scraper_task())
    asyncio.create_task(checker_task())
    
    # –°–∞–º vacuum_job –≤–∏—Å–∏—Ç –≤–µ—á–Ω–æ, —á—Ç–æ–±—ã —Ç–∞—Å–∫ –Ω–µ –∑–∞–∫—Ä—ã–ª—Å—è
    while True:
        await asyncio.sleep(3600)
